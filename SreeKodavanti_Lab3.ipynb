{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "torch.manual_seed(7)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 38\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "# Load training dataset into a single batch to compute mean and stddev.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
        "data = next(iter(trainloader))\n",
        "mean = data[0].mean()\n",
        "stddev = data[0].std()\n",
        "\n",
        "# Helper function needed to standardize data when loading datasets.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(mean, stddev)])\n",
        "\n",
        "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
        "testset = MNIST(root='./pt_data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a Sequential (feed-forward) model.\n",
        "# 784 inputs.\n",
        "# Two fully-connected layers with 25 and 10 neurons.\n",
        "# tanh as activation function for hidden layer.\n",
        "# Logistic (sigmoid) as activation function for output layer.\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 25),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(25, 10),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Initialize weights.\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
        "        nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "# Use stochastic gradient descent (SGD) with\n",
        "# learning rate of 0.01 and no other bells and whistles.\n",
        "# MSE as loss function.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Transfer model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Create DataLoader objects that will help create mini-batches.\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Train the model. In PyTorch we have to implement the training loop ourselves.\n",
        "for i in range(EPOCHS):\n",
        "    model.train() # Set model in training mode.\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_batches = 0\n",
        "    for inputs, targets in trainloader:\n",
        "        # Move data to GPU.\n",
        "        one_hot_targets = nn.functional.one_hot(targets, num_classes=10).float()\n",
        "        inputs, targets, one_hot_targets = inputs.to(device), targets.to(device), one_hot_targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass.\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, one_hot_targets)\n",
        "\n",
        "        # Accumulate metrics.\n",
        "        _, indices = torch.max(outputs.data, 1)\n",
        "        train_correct += (indices == targets).sum().item()\n",
        "        train_batches +=  1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and update.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / train_batches\n",
        "    train_acc = train_correct / (train_batches * BATCH_SIZE)\n",
        "\n",
        "    # Evaluate the model on the test dataset. Identical to loop above but without\n",
        "    # weight adjustment.\n",
        "    model.eval() # Set model in inference mode.\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_batches = 0\n",
        "    for inputs, targets in testloader:\n",
        "        one_hot_targets = nn.functional.one_hot(targets, num_classes=10).float()\n",
        "        inputs, targets, one_hot_targets = inputs.to(device), targets.to(device), one_hot_targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, one_hot_targets)\n",
        "        _, indices = torch.max(outputs, 1)\n",
        "        test_correct += (indices == targets).sum().item()\n",
        "        test_batches +=  1\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    test_loss = test_loss / test_batches\n",
        "    test_acc = test_correct / (test_batches * BATCH_SIZE)\n",
        "\n",
        "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9_s-UyFzpDA",
        "outputId": "1858f8bb-75a8-42ca-91f1-72ca2c807f36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./pt_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17612856.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./pt_data/MNIST/raw/train-images-idx3-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 478602.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./pt_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./pt_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3773037.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./pt_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3067718.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./pt_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Epoch 1/38 loss: 0.1185 - acc: 0.2432 - val_loss: 0.0892 - val_acc: 0.3066\n",
            "Epoch 2/38 loss: 0.0873 - acc: 0.3195 - val_loss: 0.0855 - val_acc: 0.3364\n",
            "Epoch 3/38 loss: 0.0835 - acc: 0.3557 - val_loss: 0.0811 - val_acc: 0.3793\n",
            "Epoch 4/38 loss: 0.0786 - acc: 0.4022 - val_loss: 0.0757 - val_acc: 0.4310\n",
            "Epoch 5/38 loss: 0.0733 - acc: 0.4550 - val_loss: 0.0704 - val_acc: 0.4842\n",
            "Epoch 6/38 loss: 0.0683 - acc: 0.5115 - val_loss: 0.0657 - val_acc: 0.5354\n",
            "Epoch 7/38 loss: 0.0639 - acc: 0.5668 - val_loss: 0.0614 - val_acc: 0.5948\n",
            "Epoch 8/38 loss: 0.0598 - acc: 0.6293 - val_loss: 0.0574 - val_acc: 0.6610\n",
            "Epoch 9/38 loss: 0.0560 - acc: 0.6857 - val_loss: 0.0536 - val_acc: 0.7147\n",
            "Epoch 10/38 loss: 0.0524 - acc: 0.7307 - val_loss: 0.0500 - val_acc: 0.7556\n",
            "Epoch 11/38 loss: 0.0490 - acc: 0.7646 - val_loss: 0.0468 - val_acc: 0.7844\n",
            "Epoch 12/38 loss: 0.0460 - acc: 0.7908 - val_loss: 0.0438 - val_acc: 0.8110\n",
            "Epoch 13/38 loss: 0.0431 - acc: 0.8154 - val_loss: 0.0410 - val_acc: 0.8323\n",
            "Epoch 14/38 loss: 0.0405 - acc: 0.8338 - val_loss: 0.0384 - val_acc: 0.8470\n",
            "Epoch 15/38 loss: 0.0381 - acc: 0.8464 - val_loss: 0.0361 - val_acc: 0.8585\n",
            "Epoch 16/38 loss: 0.0359 - acc: 0.8562 - val_loss: 0.0341 - val_acc: 0.8685\n",
            "Epoch 17/38 loss: 0.0340 - acc: 0.8650 - val_loss: 0.0322 - val_acc: 0.8758\n",
            "Epoch 18/38 loss: 0.0322 - acc: 0.8710 - val_loss: 0.0305 - val_acc: 0.8798\n",
            "Epoch 19/38 loss: 0.0307 - acc: 0.8763 - val_loss: 0.0291 - val_acc: 0.8844\n",
            "Epoch 20/38 loss: 0.0293 - acc: 0.8803 - val_loss: 0.0277 - val_acc: 0.8874\n",
            "Epoch 21/38 loss: 0.0280 - acc: 0.8843 - val_loss: 0.0266 - val_acc: 0.8907\n",
            "Epoch 22/38 loss: 0.0269 - acc: 0.8875 - val_loss: 0.0255 - val_acc: 0.8931\n",
            "Epoch 23/38 loss: 0.0259 - acc: 0.8898 - val_loss: 0.0246 - val_acc: 0.8957\n",
            "Epoch 24/38 loss: 0.0250 - acc: 0.8919 - val_loss: 0.0238 - val_acc: 0.8976\n",
            "Epoch 25/38 loss: 0.0242 - acc: 0.8941 - val_loss: 0.0230 - val_acc: 0.8996\n",
            "Epoch 26/38 loss: 0.0235 - acc: 0.8958 - val_loss: 0.0224 - val_acc: 0.9014\n",
            "Epoch 27/38 loss: 0.0229 - acc: 0.8974 - val_loss: 0.0218 - val_acc: 0.9018\n",
            "Epoch 28/38 loss: 0.0223 - acc: 0.8991 - val_loss: 0.0212 - val_acc: 0.9023\n",
            "Epoch 29/38 loss: 0.0217 - acc: 0.9001 - val_loss: 0.0207 - val_acc: 0.9040\n",
            "Epoch 30/38 loss: 0.0213 - acc: 0.9017 - val_loss: 0.0203 - val_acc: 0.9050\n",
            "Epoch 31/38 loss: 0.0208 - acc: 0.9030 - val_loss: 0.0199 - val_acc: 0.9059\n",
            "Epoch 32/38 loss: 0.0204 - acc: 0.9045 - val_loss: 0.0195 - val_acc: 0.9073\n",
            "Epoch 33/38 loss: 0.0200 - acc: 0.9057 - val_loss: 0.0191 - val_acc: 0.9081\n",
            "Epoch 34/38 loss: 0.0197 - acc: 0.9067 - val_loss: 0.0188 - val_acc: 0.9092\n",
            "Epoch 35/38 loss: 0.0193 - acc: 0.9075 - val_loss: 0.0185 - val_acc: 0.9097\n",
            "Epoch 36/38 loss: 0.0190 - acc: 0.9082 - val_loss: 0.0182 - val_acc: 0.9115\n",
            "Epoch 37/38 loss: 0.0187 - acc: 0.9090 - val_loss: 0.0180 - val_acc: 0.9120\n",
            "Epoch 38/38 loss: 0.0185 - acc: 0.9096 - val_loss: 0.0177 - val_acc: 0.9126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters."
      ],
      "metadata": {
        "id": "N24ondkfSZBA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_OwLgkaVaal"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}